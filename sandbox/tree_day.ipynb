{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f298c2af-18f6-4041-8cfd-2188a40bb289",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3f8538-eb63-4a12-8df6-d3323c11a1f9",
   "metadata": {},
   "source": [
    "## Entropie de Shannon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aaa0d022",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = {\"col_1\": [\"toto\", \"titi\", \"tata\", \"titi\", \"toto\", \"tata\", \"toto\", \"titi\"],\n",
    "\"col_2\": [\"toto\", \"titi\", \"tata\", \"titi\", \"toto\", \"tata\", \"toto\", \"titi\"],\n",
    "\"col_3\": [1, 3, 2, 7, 9, 0, 2, 5],\n",
    "\"col_4\": [1.2, 2.4, 3.7, 2.1, 0.2, 9.2, 5.4, 5.3],}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbec068-b5c2-4b46-80ef-5da8c95a34b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(array):\n",
    "    \"\"\" Computes the Shannon entropy of a non empty numpy array.\n",
    "    \n",
    "    Args:\n",
    "        array (np.ndarray): \n",
    "        \n",
    "    Return:\n",
    "        entropy(float): shannon's entropy as float or None if input is not a non-empty array\n",
    "    \"\"\"\n",
    "    if isinstance(array, np.ndarray) and (len(array) != 0):\n",
    "        classes, count_per_class = np.unique(array, return_counts=True)\n",
    "        total = len(array)\n",
    "        p = np.array([n / total for n in count_per_class])\n",
    "        #print(f\"p_i = {p}\")\n",
    "        entropy = - np.sum([pi * np.log2(pi) for pi in p]) \n",
    "        return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0a8034-a10d-4eac-ae27-9d31d6a540f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_1 = np.array([\"cat\", \"dog\", \"cat\", \"bird\", \"dog\", \"cat\", \"bird\", \"dog\", \"bird\", \"cat\"])\n",
    "arr_2 = np.array([1, 2, 3, 2, 1, 1, 1, 2, 3, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43a13b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_1 == 'bird'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d72eb5-a287-48b3-923a-d0fc0022553e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Entropy pour l'array []:\", entropy(np.array([])))\n",
    "print(\"Entropy pour l'array {1,2}:\", entropy({1,2}))\n",
    "print(\"Entropy pour l'array bob:\", entropy(\"bob\"))\n",
    "print(\"Entropy pour l'array [0, 0, 0, 0, 0, 0]:\", entropy(np.array([0, 0, 0, 0, 0, 0])))\n",
    "print(\"Entropy pour l'array [6]: \", entropy(np.array([6])))\n",
    "print(\"Entropy pour l'array ['a','a', 'b', 'b']: \", entropy(np.array(['a','a', 'b', 'b'])))\n",
    "print(\"Entropy pour l'array ['0', '0', '1', '0', 'bob', '1']: \", entropy(np.array(['0', '0', '1', '0', 'bob', '1'])))\n",
    "print(\"Entropy pour l'array [0, 0, 1, 0, 2, 1]: \", entropy(np.array([0, 0, 1, 0, 2, 1])))\n",
    "print(\"Entropy pour l'array ['0', 'bob', '1']: \", entropy(np.array(['0', 'bob', '1'])))\n",
    "print(\"Entropy pour l'array [1, 1, 1, 1, 1, 1, 1, 1, 1]: \", entropy(np.array([1, 1, 1, 1, 1, 1, 1, 1, 1])))\n",
    "print(\"Entropy pour l'array [0, 1, 1, 1, 1, 1, 1, 1, 1]: \", entropy(np.array([0, 1, 1, 1, 1, 1, 1, 1, 1])))\n",
    "print(\"Entropy pour l'array [0, 1, 1]: \", entropy(np.array([0, 1, 1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760c831d-ca48-474d-b9cd-957177893fa6",
   "metadata": {},
   "source": [
    "## Gini entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cee7d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini(array):\n",
    "    \"\"\" Computes the Gini impurity indexof a non empty numpy array.\n",
    "    \n",
    "    Args:\n",
    "        array (np.ndarray): \n",
    "        \n",
    "    Return:\n",
    "        entropy(float): Gini impurity index as float\n",
    "                        None if input is not a non-empty array\n",
    "    \"\"\"\n",
    "    if isinstance(array, np.ndarray) and (len(array) != 0):\n",
    "        classes, count_per_class = np.unique(array, return_counts=True)\n",
    "        total = len(array)\n",
    "        p = np.array([n / total for n in count_per_class])\n",
    "        #print(f\"p_i = {p}\")\n",
    "        gini_index = 1 - np.sum([pi ** 2 for pi in p]) \n",
    "        return gini_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b7eaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Gini pour l'array []:\", gini(np.array([])))\n",
    "print(\"Gini pour l'array {1,2}:\", gini({1,2}))\n",
    "print(\"Gini pour l'array bob:\", gini(\"bob\"))\n",
    "print(\"Gini pour l'array [0, 0, 0, 0, 0, 0]:\", gini(np.array([0, 0, 0, 0, 0, 0])))\n",
    "print(\"Gini pour l'array [6]: \", gini(np.array([6])))\n",
    "print(\"Gini pour l'array ['a','a', 'b', 'b']: \", gini(np.array(['a','b', 'c', 'd', 'e'])))\n",
    "print(\"Gini pour l'array ['0', '0', '1', '0', 'bob', '1']: \", gini(np.array(['0', '0', '1', '0', 'bob', '1'])))\n",
    "print(\"Gini pour l'array [0, 0, 1, 0, 2, 1]: \", gini(np.array([0, 0, 1, 0, 2, 1])))\n",
    "print(\"Gini pour l'array ['0', 'bob', '1']: \", gini(np.array(['0', 'bob', '1'])))\n",
    "print(\"Gini pour l'array [1, 1, 1, 1, 1, 1, 1, 1, 1]: \", gini(np.array([1, 1, 1, 1, 1, 1, 1, 1, 1])))\n",
    "print(\"Gini pour l'array [0, 1, 1, 1, 1, 1, 1, 1, 1]: \", gini(np.array([0, 1, 1, 1, 1, 1, 1, 1, 1])))\n",
    "print(\"Gini pour l'array [0, 1, 1]: \", gini(np.array([0, 1, 1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa89039d-e17b-4126-b090-023a8c878151",
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_gain(arr_source, arr_child, criterion='gini'):\n",
    "    \"\"\"\n",
    "    Computes the information gain between the first and second\n",
    "    array using the criterion ['gini', 'entropy']\n",
    "    \n",
    "    Args:\n",
    "        arr_source (np.ndarray):  \n",
    "        arr_child (np.ndarray): \n",
    "        criterion (str): \n",
    "    Return:\n",
    "        (float):  Shannon entropy as a float or None\n",
    "            if input is not a non-empty array\n",
    "        None: if invalid input\n",
    "    \"\"\"\n",
    "    #print(\"ici 1: information gain\")\n",
    "    if not (isinstance(arr_source, np.ndarray) and (len(arr_source) != 0)):\n",
    "        #raise ValueError('arr_source is not of expected type or length')\n",
    "        print('arr_source is not of expected type or length')\n",
    "        return None\n",
    "    #print(\"ici 1-a: information gain\")\n",
    "    if not (isinstance(arr_child,list) and (len(arr_child) != 0)):\n",
    "        #raise ValueError('arr_child is not of expected type or length')\n",
    "        print('arr_child is not of expected type (list) or length')\n",
    "        return None\n",
    "    #print(\"ici 1-b: information gain\")\n",
    "    if not all([isinstance(arr, np.ndarray) for arr in arr_child]):\n",
    "        #raise ValueError('arr_child element is not of expected type (np.ndarray) or length')\n",
    "        print('At least one of arr_child element is not of expected type (np.ndarray) or length')\n",
    "        return None\n",
    "    #print(\"ici 1-c: information gain\")\n",
    "    if criterion == 'gini':\n",
    "        f_information = gini\n",
    "    elif criterion == 'entropy':\n",
    "        f_information = entropy\n",
    "    else:\n",
    "        raise ValueError(f\"criterion is '{criterion}', one may choose between ['gini', 'entropy']\")\n",
    "    \n",
    "    #print(\"ici 2: information gain\")\n",
    "    classes_source, count_per_class_src = np.unique(arr_source, return_counts=True) \n",
    "    classes_child, count_per_class_child = np.unique(np.concatenate(arr_child), return_counts=True)\n",
    "    #print(\"len(classes_child): \", len(classes_child))\n",
    "    #print(\"count_per_class_child: \", count_per_class_child)\n",
    "    #print(\"ici 3: information gain\")\n",
    "    if not all([c in classes_source for c in classes_child]):\n",
    "        #print(\"<TMP> classes_source: \", classes_source)\n",
    "        #print(\"<TMP> classes_child: \", classes_child)\n",
    "        # raise ValueError('At least one child array has a class not in the source array.')\n",
    "        print('At least one child array has a class not in the source array. Or there is a missing class in children arrays.')\n",
    "        return None\n",
    "    #print(\"ici 4: information gain\")\n",
    "    if np.sum(count_per_class_child) != np.sum(count_per_class_src):\n",
    "        # raise ValueError('At least one child array has a class not in the source array.')\n",
    "        print('Splitting is not conservative. Total element in source is not equal to sum of children arrays.')\n",
    "        return None\n",
    "    #print(\"ici 5: information gain\")\n",
    "    S0 = f_information(arr_source)\n",
    "    #print(\"ici 6: information gain\")\n",
    "    S = [f_information(arr) for arr in arr_child]\n",
    "    p = [ len(arr) / len(arr_source) for arr in arr_child]\n",
    "    #print(\"ici 7: information gain\")\n",
    "    information_gain = S0 - np.sum([p_i * s_i for p_i, s_i in zip(p,S)])\n",
    "    return information_gain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9db27e7-7c72-4b3e-acd2-24bb134afffc",
   "metadata": {},
   "source": [
    "\n",
    "Information gain between [] and [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] is None with\n",
    "criterion 'gini' and None with criterion 'entropy'\n",
    "\n",
    "Information gain between ['a' 'a' 'b' 'b'] and {1, 2} is None with criterion\n",
    "'gini' and None with criterion 'entropy'\n",
    "\n",
    "Information gain between [0. 1. 1. 1. 1. 1. 1. 1. 1. 1.] and [1. 1. 1. 1. 1.\n",
    "1. 1. 1. 1. 1.] is 0.18 with criterion 'gini' and 0.4689955935892812 with\n",
    "criterion 'entropy'\n",
    "\n",
    "Information gain between ['0' '0' '1' '0' 'bob' '1'] and [array(['0', 'bob',\n",
    "'1'], dtype='Information gain between ['0' '0' '1' '0' 'bob' '1'] and [0 0 1\n",
    "0 2 1] is 0.0 with criterion 'gini' and 0.0 with criterion 'entropy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61975a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple 1:\n",
    "print(\"# Exemple 1:\")\n",
    "arr_source = np.array([])\n",
    "arr_child = np.array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
    "\n",
    "print(information_gain(arr_source, arr_child, criterion='gini'))\n",
    "print(information_gain(arr_source, arr_child, criterion='entropy'))\n",
    "\n",
    "# Exemple 2:\n",
    "print(\"# Exemple 2:\")\n",
    "arr_source = np.array(['a', 'a', 'b', 'b'])\n",
    "arr_child = [np.array(['a', 'b']), np.array(['a', 'b'])]\n",
    "\n",
    "print(information_gain(arr_source, arr_child, criterion='gini'))\n",
    "print(information_gain(arr_source, arr_child, criterion='entropy'))\n",
    "\n",
    "# Exemple 3:\n",
    "print(\"# Exemple 3:\")\n",
    "arr_source = np.array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
    "arr_child = [np.array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])]\n",
    "\n",
    "print(information_gain(arr_source, arr_child, criterion='gini'))\n",
    "print(information_gain(arr_source, arr_child, criterion='entropy'))\n",
    "\n",
    "# Exemple 4:\n",
    "print(\"# Exemple 4:\")\n",
    "arr_source = np.array(['0', '0', '1', '0', 'bob', '1'])\n",
    "arr_child = [np.array(['0', '0', '0']), np.array(['1', 'bob', '1'])]\n",
    "\n",
    "print(information_gain(arr_source, arr_child, criterion='gini'))\n",
    "print(information_gain(arr_source, arr_child, criterion='entropy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd6b7b6-41ab-423b-a3ac-b5e8e3262c8f",
   "metadata": {},
   "source": [
    "## Information Gain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75887b97-cd37-4fb3-a202-a1f1fd6f5a36",
   "metadata": {},
   "source": [
    "## Understanding the Mathematical Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411b1fd9-e714-4a3b-8377-f3de926796e4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "322595ea",
   "metadata": {},
   "source": [
    "> #### Question 1:\n",
    "> *Define what Gini impurity is about and what it measures. (No mathematical formula, just the general concept in words).*\n",
    ">\n",
    "> lorem ipsum\n",
    "\n",
    "> #### Question 2:\n",
    "> *Define what Shannon entropy is and what it measures. (No mathematical formula, just the general concept in words).*\n",
    ">\n",
    "> lorem ipsum\n",
    "\n",
    "> #### Question 3:\n",
    "> *Define what Information gain is and what it measures. (No mathematical formula, just the general concept in words).*\n",
    ">\n",
    "> lorem ipsum\n",
    "\n",
    "> #### Question 4:\n",
    "> *Explain how these 3 concepts are used for decision trees.*\n",
    ">\n",
    "> lorem ipsum\n",
    "\n",
    "> #### Question 5:\n",
    "> *If the dataset has 2 classes, explain what are the boundaries (minimum and maximum) of Gini impurity and Shannon entropy.*\n",
    ">\n",
    "> lorem ipsum\n",
    "\n",
    "> #### Question 6:\n",
    "> *What does it mean if the Gini impurity is 0? If Shannon entropy is 0 ?*\n",
    ">\n",
    "> lorem ipsum\n",
    "\n",
    "> #### Question 7:\n",
    "> *Why should you use Gini impurity as default?*\n",
    ">\n",
    "> lorem ipsum\n",
    "\n",
    "> #### Question 8:\n",
    "> *What are the pros and cons of just using the criteria Information Gain positive or negative to pick the feature of a decision tree? Can we mitigate this risk ?*\n",
    ">\n",
    "> lorem ipsum\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bef7232-5fd3-4958-b039-634f5f0f4564",
   "metadata": {},
   "source": [
    "## Understanding better Decision for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6aa16e-19b5-46e7-9668-42344f6cfb1c",
   "metadata": {},
   "source": [
    "### Answers:\n",
    "\n",
    "> #### Question 1:\n",
    "> *What are the pros of using decision trees?*\n",
    ">\n",
    "> lorem ipsum\n",
    "\n",
    "> #### Question 2:\n",
    "> *What are the cons of using decision trees?*\n",
    ">\n",
    "> lorem ipsum\n",
    "\n",
    "> #### Question 3:\n",
    "> *What is overfitting? How does it apply to decision trees?*\n",
    ">\n",
    "> lorem ipsum\n",
    "\n",
    "> #### Question 4:\n",
    "> *What can be done to avoid overfitting in decision trees?*\n",
    ">\n",
    "> lorem ipsum\n",
    "\n",
    "> #### Question 5:\n",
    "> *What is the name of the algorithm used by sklearn for classification decision trees?*\n",
    ">\n",
    "> lorem ipsum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b2b923-5d8d-410a-b54d-7b936743c982",
   "metadata": {},
   "source": [
    "## Exercise 05: Code the DecitionTreeClassifier Fit Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb180fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea1bba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \"\"\"\n",
    "    attributes:\n",
    "        data [pandas DataFrame]: dataframe containing the features\n",
    "        labels [pandas.Dataframe]: labels\n",
    "        is_leaf [bool]: True if the node is a leaf of the tree\n",
    "        split_feature [int]: column of the feature\n",
    "        split_kind [str]: \"<=\" or \"=\"\n",
    "        split_criteria [float]: value of the criteria used to split data\n",
    "        left [Node]: node child where criteria is True\n",
    "        right [Node]: node child where criteria is False\n",
    "        depth [int]: depth level of the node in the tree\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "        data=None,\n",
    "        labels=None,\n",
    "        is_leaf=False,\n",
    "        split_feature=None,\n",
    "        split_kind=None,\n",
    "        split_criteria=None,\n",
    "        left=None,\n",
    "        right=None,\n",
    "        depth=0):\n",
    "        # data\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "        # split_info\n",
    "        self.is_leaf = is_leaf\n",
    "        self.split_feature = split_feature\n",
    "        self.split_kind = split_kind\n",
    "        self.split_criteria = split_criteria\n",
    "        if self.is_leaf:\n",
    "            self.content = \"Leaf\"\n",
    "        else:\n",
    "            self.content = f\"Feature {self.split_feature} {self.split_kind} {self.split_criteria}\"\n",
    "\n",
    "        # children\n",
    "        self.left_child = left\n",
    "        self.right_child = right\n",
    "\n",
    "        # meta\n",
    "        self.depth = depth\n",
    "\n",
    "    def __str__(self):\n",
    "        output_print = \"\"\"{}\\nNode depth = {}\\n\\n\"\"\".format(self.content, self.depth)\n",
    "        if self.is_leaf:\n",
    "            output_print += \"\"\"X =\\n{}\\n\\ny = \\n{}\\n\"\"\".format(self.data, self.labels)\n",
    "        return output_print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d761c0e6-0b03-410e-ad21-1231db086575",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import Union, List\n",
    "\n",
    "map_operator_kind = {\"==\": bool.__eq__, \"<=\": float.__gt__}\n",
    "\n",
    "class DecisionTreeClassifier:\n",
    "    def __init__(self, criterion='gini', max_depth=10):\n",
    "        \"\"\" Constructor, with stored attributes.\n",
    "        \n",
    "        Args:\n",
    "            criterion (str): 'gini' or 'entropy'\n",
    "            max_depth (int): max_depth of the tree\n",
    "                        (Decision tree creation\n",
    "                        -stops splitting a node if node.depth >= max_depth)\n",
    "        \"\"\"\n",
    "        self.root = None # Root node of the tree\n",
    "        self.criterion = criterion\n",
    "        if criterion == \"gini\":\n",
    "            self.ft_impurity = gini\n",
    "        else:\n",
    "            self.ft_impurity = entropy\n",
    "        self.max_depth = max_depth\n",
    "        # Your code here. You can add more things if needed\n",
    "    \n",
    "    def _search_best_split_values_(self, X: pd.Series, y: pd.Series) -> float:\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            X (pd.Series): _description_\n",
    "            y (pd.Series): _description_\n",
    "\n",
    "        Returns:\n",
    "            float: _description_\n",
    "        \"\"\"\n",
    "        #print(\"<_search_best_split_values_>: 0\")\n",
    "        X_sorted = X.sort_values()\n",
    "        y_sorted = y.iloc[X_sorted.index]\n",
    "        #print(y_sorted)\n",
    "        ig_max = 0\n",
    "        #print(\"<_search_best_split_values_>: 2\")\n",
    "        x_unique = X_sorted.unique()\n",
    "        # Il manque les valeurs interm√©diaire entre chaque valeurs de x_unique\n",
    "        # dic_split_ig = {\"split_value\": [], \"IG_value\": []}\n",
    "        split_value = 0\n",
    "        for x_i in x_unique[:-1]:\n",
    "            #print(\"<_search_best_split_values_>: 3\")\n",
    "            ig_tmp = information_gain(\n",
    "                y_sorted.values,\n",
    "                [\n",
    "                    y_sorted[X_sorted <= x_i].values,\n",
    "                    y_sorted[X_sorted > x_i].values\n",
    "                ],\n",
    "                criterion=self.criterion)\n",
    "            #print(\"<_search_best_split_values_>: 4\") \n",
    "            #dic_split_ig[\"split_value\"].append(x_i)\n",
    "            #dic_split_ig[\"IG_value\"].append(ig_tmp)\n",
    "            if ig_tmp > ig_max:\n",
    "                ig_max = ig_tmp\n",
    "                split_value = x_i\n",
    "            #print(\"<_search_best_split_values_>: 5\")\n",
    "        #df = pd.DataFrame(data=dic_split_ig)\n",
    "        return split_value # df\n",
    "\n",
    "    def _gen_children_nodes_(\n",
    "        self,\n",
    "        node: Node,\n",
    "        feature_name: str\n",
    "        ):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            node (Node): _description_\n",
    "            X (pd.Dataframe): _description_\n",
    "            feature_name (Union[np.int_, np.str_, np.bool_]): _description_\n",
    "        \"\"\"\n",
    "        #print(\"<_gen_children_nodes_>: 0\")\n",
    "        split_val = self._search_best_split_values_(node.data[feature_name].copy(), node.labels.copy()) \n",
    "        #print(\"<_gen_children_nodes_>: 1\")\n",
    "        if node.split_kind == \"==\":\n",
    "            #print(\"<_gen_children_nodes_>: 2\")\n",
    "            idx_X_left = node.data[feature_name] == split_val\n",
    "            #print(\"<_gen_children_nodes_>: 3\")\n",
    "            idx_X_right = node.data[feature_name] != split_val\n",
    "        else:\n",
    "            #print(\"<_gen_children_nodes_>: 4\")\n",
    "            idx_X_left = node.data[feature_name] <= split_val\n",
    "            #print(\"<_gen_children_nodes_>: 5\")\n",
    "            idx_X_right = node.data[feature_name] > split_val\n",
    "        #print(\"<_gen_children_nodes_>: generating left_node ...\")\n",
    "        left_node = Node(\n",
    "            data=node.data.loc[idx_X_left].copy(),\n",
    "            labels=node.labels.loc[idx_X_left].copy(),\n",
    "            is_leaf=True,\n",
    "            split_feature=feature_name,\n",
    "            split_kind=node.split_kind,\n",
    "            split_criteria=split_val,\n",
    "            left=None,\n",
    "            right=None,\n",
    "            depth= node.depth + 1\n",
    "            )\n",
    "        left_node.impurity = self.ft_impurity(left_node.labels)\n",
    "        if left_node.impurity == 0.0:\n",
    "            left_node.is_impure = False\n",
    "        else:\n",
    "            left_node.is_impure = True\n",
    "        #print(\"<_gen_children_nodes_>: generating right_node ...\")\n",
    "        right_node = Node(\n",
    "            data=node.data.loc[idx_X_right].copy(),\n",
    "            labels=node.labels.loc[idx_X_right].copy(),\n",
    "            is_leaf=True,\n",
    "            split_feature=feature_name,\n",
    "            split_kind=node.split_kind,\n",
    "            split_criteria=split_val,\n",
    "            left=None,\n",
    "            right=None,\n",
    "            depth=node.depth + 1\n",
    "            )\n",
    "        right_node.impurity = self.ft_impurity(right_node.labels)\n",
    "        if right_node.impurity == 0.0:\n",
    "            right_node.is_impure = False\n",
    "        else:\n",
    "            right_node.is_impure = True\n",
    "\n",
    "        node.is_leaf = False\n",
    "        node.left = left_node\n",
    "        node.right = right_node\n",
    "        return left_node, right_node\n",
    "\n",
    "    \n",
    "    def build_tree(self, node: Node):\n",
    "        if node.depth == 0:\n",
    "            print(\"profondeur maximale: \", self.max_depth)\n",
    "        print(\"profondeur du node: \", node.depth)\n",
    "\n",
    "        if node.is_impure and (node.depth < self.max_depth):\n",
    "            draw_feature = random.choice(node.data.columns.to_list())\n",
    "            #print(\"<INFO>: constructing the left and right nodes ...\")\n",
    "            left_node, right_node = self._gen_children_nodes_(node, draw_feature)\n",
    "            #print(\"<INFO>: constructing the left and right nodes: DONE\")\n",
    "            if left_node.is_impure:\n",
    "                self.build_tree(left_node)\n",
    "            if left_node.is_impure:\n",
    "                self.build_tree(right_node)\n",
    "        else:\n",
    "            node.is_leaf =True\n",
    "    \n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Build the decision tree from the training set (X, y).\n",
    "        The training set has m data_points (examples).\n",
    "        Each of them has n features.\n",
    "        \n",
    "        Args:\n",
    "            X (pandas.Dataframe): Training input (m x n)\n",
    "            y (pandas.Serie): Labels (m x 1)\n",
    "        \n",
    "        Return:\n",
    "            self: Trained tree\n",
    "        \"\"\"\n",
    "        \n",
    "        # Determination des types de chaque colonne:\n",
    "        \n",
    "        y_type = X.dtypes.values\n",
    "        #if y_type != np.str_:\n",
    "        #    raise TypeError(\"type of target must be string.\")\n",
    "        \n",
    "        self.root = Node(\n",
    "            data=X.copy(),\n",
    "            labels=y.copy(),\n",
    "            is_leaf=False,\n",
    "            split_feature=None,\n",
    "            split_kind=None,\n",
    "            split_criteria=None,\n",
    "            left=None,\n",
    "            right=None,\n",
    "            depth=0\n",
    "            )\n",
    "        info_gain = self.root.impurity = self.ft_impurity(self.root.labels) \n",
    "        if info_gain == 0:\n",
    "            self.root.is_impure = False\n",
    "        else:\n",
    "            self.root.is_impure = True\n",
    "        \n",
    "        for feature_name in self.root.data.columns.to_list():\n",
    "            #print(f\"valeur de feature name: {feature_name} -- dtype correspondant: \", X[feature_name].dtype)\n",
    "            #print(isinstance(X[feature_name].dtype, (float, np.float64)))\n",
    "            if isinstance(self.root.data[feature_name].dtype, bool):\n",
    "                split_kind = '=='\n",
    "                idx_X_left = self.root.data[feature_name] == True\n",
    "                idx_X_right = self.root.labels[feature_name] == False\n",
    "                split_values = True\n",
    "            else:\n",
    "                # elif isinstance(self.root.data[feature_name].dtype, (np.int_, np.float_, np.float64, np.float32)):\n",
    "                split_kind = '<='\n",
    "                split_values = self._search_best_split_values_(self.root.data[feature_name], self.root.labels)\n",
    "            #else:\n",
    "            #    raise NotImplementedError(f\"\"\"\n",
    "            #    \"The feature type is not handle '({self.root.data[feature_name].dtype})'.\n",
    "            #    Only types 'int', 'float' and 'bool' are handled.\n",
    "            #    \"\"\")\n",
    "        #print(\"<INFO>: start building the tree ...\")\n",
    "        self.build_tree(self.root)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65a7b8b-85a6-4db2-8e3e-b7bd9afddb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris(as_frame=True)\n",
    "\n",
    "X = pd.DataFrame(iris.data)\n",
    "X.rename(columns={i:iris.feature_names[i] for i in range(len(iris.feature_names))}, inplace=True)\n",
    "y = pd.DataFrame(iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92640b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0699d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0952c29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79d2ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.rename(columns={0:\"specie\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da192a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b562580",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.replace({i:iris.target_names[i] for i in range(len(iris.target_names))}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9128bdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc = DecisionTreeClassifier(max_depth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8294fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dde456",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df71e14b-4096-4292-b803-968add1f20ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn is not allowed in the classes.\n",
    "# Test on iris dataset\n",
    "# iris = load_iris()\n",
    "\n",
    "#X = pd.DataFrame(iris.data)\n",
    "#y = pd.DataFrame(iris.target)\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=1)\n",
    "\n",
    "dec_tree = DecisionTreeClassifier(max_depth=2)\n",
    "dec_tree.fit(X, y)\n",
    "root = dec_tree.root\n",
    "\n",
    "print(\"TEST ON IRIS DATASET\")\n",
    "print(f\"Root split info = 'Feature_{root.split_feature}{root.split_kind}{root.split_criteria}'\\n\")\n",
    "print(\"5 first lines of the labels of the left child of root =\\n{root.left_child.y.head()}\\n\")\n",
    "print(\"5 first lines of the labels of the right child of root =\\n{root.right_child.y.head()}\")\n",
    "#Output examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a79d223-f426-44d7-a3aa-f29fbe56bede",
   "metadata": {},
   "source": [
    "## Code the DecisionTreeClassifier Prediction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3cd551",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "light_datascience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "63d25b9b9c75c5df65fa6b1649cc2ca3b974ad8dbbf55ad3a17dc754e3238f4a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
